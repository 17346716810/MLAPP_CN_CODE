# 14.核方法

## 14.1 引言

截止目前，对于那些我们希望进行分类、聚类或者以任何其他方式处理的对象而言，都假设它们可以被一个固定长度的向量表示，即$\mathbf{x}_i \in \mathbb{R}^D$。然而，对于某些类型的目标，我们并不清楚如何以一个固定长度的向量表示它。比如说，我们如何表示诸如一个文本文件或者蛋白质序列的具备不定长度的目标？或者一个具备复杂3d结构的分子结构？或者一个具备不定长度和形状的进化树？

一种解决这类问题的方法是针对数据建立一个生成式模型，然后使用推理得到的潜在表示和（或者）模型的参数作为特征，并将这些特征用于标准的方法中。举例来说，在第28章，我们会介绍深度学习，它是一种可以学习很好的特征表示的无监督方法。

另一种方法是假设我们有一些衡量对象之间相似度的方法，在这种方法中，我们不需要将对象处理成向量形式。举例来说，当我们比较字符串时，我们可以比较它们之间的编辑距离。令$\kappa(\mathbf{x},\mathbf{x}^\prime)\ge0$为目标$\mathbf{x},\mathbf{x}^\prime\in\chi$的某种相似度测量，其中$\chi$是某个抽象的空间，我们称$\kappa$为一个**核函数(kernel function)**。需要注意的是，此处的"核(kernel)"可以表示几种含义，我们会在14.7.1节介绍一种不同的解释。

本章，我们将介绍几种核函数。然后描述几种算法，这些算法可以写成只需要核函数计算的形式。当我们无法（或者选择不去这么做）深入对象$\mathbf{x}$内部时（译者注：即我们并不关心或者无法将对象$\mathbf{x}$以固定长度表示），这种方法可以被使用。

## 14.2 核函数

我们定义一个**核函数(kernel function)**为关于两个参数的实数函数，即$\kappa(\mathbf{x},\mathbf{x}^\prime)\in\mathbb{R}$,其中$\mathbf{x},\mathbf{x}^\prime\in\chi$。其中核函数满足对称性($\kappa(\mathbf{x},\mathbf{x}^\prime)=\kappa(\mathbf{x}^\prime,\mathbf{x})$)，非负性($\kappa(\mathbf{x},\mathbf{x}^\prime)\ge 0$)，所以它可以解释为相似度的衡量，但这种解释并非必须的要求。我们将在下文给出几个例子。

### 14.2.1 RBF核

**平方指数核(squared exponential kernel, SE)**或者**高斯核(Gaussian kernel)**定义为：
$$
\kappa(\mathbf{x},\mathbf{x}^\prime)=\exp\left(-\frac{1}{2}(\mathbf{x}-\mathbf{x}^\prime)^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{x}^\prime)\right) \tag{14.1}
$$
如果$\mathbf{\Sigma}$是对角矩阵，上式可以被写成
$$
\kappa(\mathbf{x},\mathbf{x}^\prime)=\exp\left(-\frac{1}{2}\sum_{j=1}^{D}\frac{1}{\sigma_j^2}(x_j-x_j^\prime)^2\right) \tag{14.2}
$$
我们可以将$\sigma_j$解释为维度$j$的**特征长度尺度(characteristic length scale)**。如果$\sigma_j=\infty$，则对应的维度将被忽略；所以这又被称为**ARD kernel**。如果$\mathbf{\Sigma}$是球状的矩阵，我们可以得到各向同性的核函数
$$
\kappa(\mathbf{x},\mathbf{x}^\prime)=\exp\left(-\frac{||\mathbf{x}-\mathbf{x}^\prime||^2}{2\sigma^2}\right) \tag{14.3}
$$
其中$\sigma^2$被称为**带宽(bandwidth)**。式14.3是**径向基函数(radial basis function)**或者**RBF**核的一个例子，因为它只是关于$||\mathbf{x}-\mathbf{x}^\prime||$的函数。

### 14.2.2 用于比较文本的核

在我们进行文本分类或者检索时，需要有一种衡量两个文本$\mathbf{x}_i$和$\mathbf{x}_j$相似度的方法。 如果我们使用词袋法对向量进行表示，其中$x_{ij}$表示在文本$i$中单词$j$出现的次数，我们使用**余弦相似度(cosine similarity)**，定义为：
$$
\kappa(\mathbf{x}_i,\mathbf{x}_{i^\prime})=\frac{\mathbf{x}_i^T\mathbf{x}_{i^\prime}}{||\mathbf{x}_i||_2||\mathbf{x}_{i^\prime}||_2} \tag{14.4}
$$

上式表示向量$\mathbf{x}_i$和$\mathbf{x}_j$之间的夹角的余弦值。因为$\mathbf{x}_i$是一个计数向量（非负），所以余弦相似度的区间为0到1。，其中0表示向量正交，也就是说两个文本之间不存在相同的单词。

不幸的是，这种方式并不一定有效，原因有二。首先，如果$\mathbf{x}_i$与$\mathbf{x}_{i^\prime}$之间有相同的单词，那使用这种方式计算出来的结果，肯定说明两个文本是相似的，哪怕相同的单词在大部分文本中都会出现，比如"the"或者"and"，这些单词往往不具备判别性。（这些被称为**停用词(stop words)**）。其次，如果一个具有判别性的单词在文本中出现多次，那么相似度将被人为的提升，尽管单词的使用往往具有突发性(bursty)，也就是说一旦一个单词在文本中出现过一次，那么它再次被使用的概率将会很大（见3.5.5节）。

幸运的是，使用一些预处理的方式，我们可以大幅地提高性能。这种方式使用一种新的特征向量取代单词计数的向量，这种新的向量被称为**逆文档频率(TF-IDF,term frequency inverse documnet frequency)**。该向量定义方式如下，首先，定义频率为计数的函数:
$$
tf(x_{ij})=\log(1+x_{ij}) \tag{14.5}
$$
