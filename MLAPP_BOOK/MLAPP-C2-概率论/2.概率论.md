# 2.概率论

## 2.1 前沿

> Probability theory is nothing but common sense reduced to calculation. —— Pierre Laplace

在前面的章节中，我们已经了解了概率论在机器学习中所扮演的有用的角色。本章，我们将讨论更多关于概率论的细节。我们并没有足够的篇幅展开相关领域的深层次讨论——读者可以自行参考更多的相关书籍。但在后面的章节中，我们将简明扼要的介绍许多可能会用到的关键思想。

在我们探讨更多技术方面的细节之前，请容许我们思考一个问题：什么是概率？我们对于诸如“一个硬币面朝上的概率为0.5”的表述已经非常熟悉。但这句话到底意味着什么？关于概率至少有两种不同的解释。一种是**频率**（frequentist）学派的解释。在这种观点中，概率代表事件在长时间实验的情况下出现的频率。比如，在前面的例子中，我们是指如果我们投掷一枚硬币很多次，那么我们相信有一半的时间硬币的正面朝上。

另一个关于概率的解释为**贝叶斯**（Bayesian）学派的解释。在这种观点中，概率是用来量化我们对于某些事件的**不确定度**（uncertainty），所以本质上它与信息而非重复的实验相关。从贝叶斯学派的观点看待上述的例子，意味着我们相信在下一次投掷硬币时，硬币正面朝上的可能性为0.5。

贝叶斯解释的一个重要优势在于，它可以用来衡量那些无法进行重复试验的事件的不确定度。比如说我们希望估计到2020年冰川融化的概率。这个事件本身可能只发生一次甚至不会发生，也就是说它是不能被重复的试验。然而，我们可以量化针对该事件发生的不确定度（比如说基于我们采取的一些抑制全球变暖的行为，我们可以认为这个事件发生的可能性会变小）。再比如第1章中提及的垃圾邮件分类任务，我们可能已经收到一个特定的邮件信息，我们希望计算它是垃圾邮件的可能性。或者我们在雷达屏幕上观察到一个移动光点，我们希望计算这个飞行物身份的概率分布（它是一只鸟还是一个飞机呢？）。在上述的所有案例中，尽管没有一个事件是可以重复试验的，但贝叶斯观点却是有效且具备天然可解释性的。**所以在本书中我们将采用贝叶斯观点对概率的解释。**不过幸运的是，无论我们采取哪一种观点去看待概率，概率论的基本规则都是一样的。

## 2.2 关于概率论的简单综述

本章介绍关于概率论基础的一些知识，仅仅针对那些对相关知识已经生疏的读者。关于更多的相关细节，可以参考其他的相关书籍。已经对这块知识比较熟悉的读者可以直接跳过本章。

### 2.2.1 离散随机变量

表达式$p(A)$表示事件$A$发生的概率。比如$A$可能表示"明天会下雨"。其中$p(A)$满足$0\le p(A)\le1$，如果$p(A)=0$，表示事件$A$不可能发生，$p(A)=1$意味着事件$A$肯定发生。我们使用$p(\bar A)$表示事件非$A$发生的可能性，满足$p(\bar A)=1-p(A)$。通常情况下，我们将"$A$发生"这个事件写作$A=1$，"$A$不发生"写作$A=0$。

通过定义**离散随机变量**（discrete random variable）$X$，我们可以扩展二元事件（即事件只存在两种状态）的符号表达，该离散变量取值于一个有限集或者可数无限集 $\chi$(译者注：关于可数无限集的例子：比如做一个抛掷硬币的试验，直到第一次出现正面时抛掷硬币的次数$\chi$的取值所构成的就是一个可数无限集)。我们将事件$X=x$发生的概率表示为$p(X=x)$，或者直接写成$p(x)$。其中符号$p()$称为**概率质量函数**(probability mass function),满足性质$0 \le p(x) \le 1,\sum_{x\in\chi}p(x)=1$。图2.1展示了定义在一个有限**状态空间**(state space)$\chi = \{1,2,3,4\}$上的两种概率质量函数。其中左图属于均匀分布,$p(x)=1/4$，右图为一个退化分布$p(x)=\mathbb{I}(x=1)$，其中$\mathbb{I}()$为二元**指示函数**(indicator function)，这个分布意味着$X$永远等于1，换句话说，它是一个常数。

### 2.2.2 基本定理

本章，我们将介绍概率论的基本定理。

#### 2.2.2.1 两个事件并集发生的概率 

给定两个事件$A$和$B$，定义事件$A$或$B$发生的概率为：
$$
\begin{align}
p(A\or B) &= p(A)+p(B)-p(A\or B)  \tag{2.1} \\
&=p(A)+p(B)  \ 如果A和B互斥         \tag{2.2} \\
\end{align} 
$$

#### 2.2.2.2 联合概率

我们定义事件$A$和$B$同时发生的概率为：
$$
p(A,B)=p(A\and B)=p(A|B)p(B) \tag{2.3}
$$
上式通常又被称为**乘法法则**(product rule)。给定两个事件的**联合概率分布**（joint distribution）$p(A,B)$，定义**边缘分布**(marginal distribution)如下: 
$$
p(A)=\sum_b p(A,B)=\sum_b p(A|B=b)p(B=b) \tag{2.4}
$$
上式我们针对事件$B$所有的可能状态进行求和。类似地，我们也可以定义$p(B)$，这通常被称为**求和法则**（sum rule）或者叫**全概率法则**（rule of total probability）。

我们可以多次使用乘法法则，进而引出概率论中的**链式法则**（chain rule）：
$$
p(X_{1:D})=p(X_1)p(X_2|X_1)p(X_3|X_2,X_1)...p(X_D|X_{1:D-1}) \tag{2.5}
$$
式中，我们模仿了Matlab中的一种符号写法$1:D$表示集合$\{1,2,…,D\}$。

#### 2.2.2.3 条件概率

我们定义在事件$B$发生的前提下，事件$A$发生的概率为**条件概率**（conditional probability）：
$$
p(A|B)=\frac{p(A,B)}{p(B)} \ {\rm{if}} \ p(B)\gt 0 \tag{2.6}
$$

### 2.2.3 贝叶斯法则

根据求和法则和求积法则，结合条件概率的定义，可以得到**贝叶斯法则**（Bayes’ rule）或者**贝叶斯定理**（Bayes’ Theorem）。
$$
p(X=x|Y=y)=\frac{p(X=x,Y=y)}{p(Y=y)}=\frac{p(X=x)p(Y=y|X=x)}{\sum_{x^\prime}p(X=x^\prime)p(Y=y|X=x^\prime)} \tag{2.7}
$$
Sir Harold Jeffreys指出“**贝叶斯定理之于概率论等价于勾股定理之于几何学**”  。我们在下面将介绍两个贝叶斯理论的应用，但在全书后面的内容中，我们将会碰到很多相关的例子。

#### 2.2.3.1 案例：医疗诊断

考虑一个关于使用贝叶斯法则的案例：医疗诊断问题。假设你是一个40多岁的女性，你决定去做一个名叫**manmogram**检测,以判断自己是否患有乳腺癌。如果检测结果显示为阳性，那么你患病的概率有多大？这个问题的答案显然与检测方法的可靠性有关。假设你被告知检测方法的**敏感性**(sensitivity)为80%，也就是说，如果你患有癌症，那么测试结果显示为阳性的可能性为0.8。换句话说：
$$
p(x=1|y=1)=0.8 \tag{2.8}
$$
其中$x=1$表示检测结果为阳性，$y=1$表示你确实患有乳腺癌。许多人因此就认为他们患有癌症的可能性为80%。但这是错误的！因为他们忽略了患有乳腺癌的先验概率，幸运的是，这个概率相当低：
$$
p(y=1)=0.004 \tag{2.9}
$$


